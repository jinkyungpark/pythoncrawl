[
{"content": "In this article, we build upon some of the semi-automated techniques and tools introduced in the  of the series.Let\u2019s say that the data we work with is separated by comma and line breaks:However, there isn\u2019t a consistency of how many words are separated by comma per line. To make it easier, we can transform the data set so there\u2019s only one word in each:In order to make this transformation of the data we can use search and replace functionalities of code text editors such as ,  or .\u00a0This how you can do it using Sublime Text:Once the process finishes, all the words will be in a single row, separated by commas:"},
{"content": "We are excited to announce our newest data extraction API. The  is now out of BETA and publicly available as a stable release.\u00a0If you are ready to roll up your sleeves and get started, here are the links you need:While this blog covers most of the notable improvements & extensive testing that the API has undergone, that warrants an exit from Beta, together with some high-level uses; it\u2019s important to remember that we have already covered it .We are moving AutoExtract Job Postings out of beta after making substantial , completely eliminating several classes of errors, and making . Aggregator websites where the API had a tendency to return failed requests on the BETA release have now been addressed, paving the way for widespread use.These changes were released to production as part of 20.5.0If you are looking to discern insights on the activities of organisations of all sizes, from start-ups to Fortune 100 companies, job postings can provide context for analysts to understand the market landscape. Where and how are competitors, suppliers and customers or even the industry, in general, structuring their business. Which technologies they are investing in, which ones they are no longer actively pursuing, what key markets are they pushing into, amongst other things."},
{"content": "Article and news data extraction is becoming increasingly popular and widely used by companies. Data quality plays a vital role in making sure these projects succeed. If the quality of the extracted articles is not good enough, your whole business could be at risk, especially if it depends on the constant flow of high quality article data.When it comes to web data extraction, data quality is always a key factor. Without high data quality, organizations face increased costs () let alone having their competitive standing undermined. If you\u2019re looking for an article extraction solution, your top priority should be data quality. You need to know which service or library provides the best article data quality. You need to learn what metrics are important when you . But also - moving further from general data quality - what measures are important in article extraction and article body extraction quality.Article body extraction quality is crucial if your business depends on this kind of data. If you\u2019re developing a product or software that needs structured article/news data constantly, you need to make sure you choose a solution which can prove they provide the best quality on the market.There are many use cases for article extraction. But one thing is common in each of them: extracting articles from the web gives you a competitive advantage that many companies fail to recognize yet. Web extracted articles and news can make youIf you want any of these skills in your arsenal, your top priority should be to choose a solution that has the best article extraction quality on the market.If you have products sold online, there\u2019s probably a lot of discussion around them as well.  their good or bad experiences of a product they bought. These mentions can decide whether future customers buy from you or they choose another brand\u2019s product. Monitoring your brand online and fueling mentions into your business intelligence can improve the way you market, promote and present your products online. It can also show you why people are buying (or not buying) your products."},
{"content": "But, first let's see why would you even need proxies. When you start extracting data from the web on a small scale you might not need proxies to make successful requests and get the data. But, as you scale your project because you need to extract more records or more frequently, you will experience issues. Or the site you're trying to reach might display different content depending on the region. So these are the two cases when you need to start using a proxy solution. proxies are much easier to get access to and they are much cheaper. In many use cases, where you cannot extract data without any proxy, you can just start using data center proxies and be able to extract data.Residential proxies are harder to get access to and they are more expensive, because they are provided by actual Internet Service Providers and not data centers. Residential proxies are also higher quality and can work even when data center proxies fail.Whether you should use data center or residential proxies in your web data extraction project, it comes down to your situation\u2019s details. There\u2019s no general rule of thumb to decide which type of proxy will work for you. But one thing is for sure: unless you have some special requirements you should start off with data center proxies. Then, based on how it works for you, you can switch to residential proxies if you really need to.Residential proxies are more expensive, thus you will probably be better off using data center proxies, if you can, and applying some techniques to keep your proxy pool clean.The biggest issue with residential proxies is, as it was mentioned, they are expensive. So usually the most effective way to scale your web data extraction project, is to try to maximize the value of data center proxies, by being smart about how you actually scrape the web and how you use proxies.Two things, that you can do to achieve this:If you want to learn more about these tactics, I recommend watching our FREE webinar on this: If you missed our webinar on the topic of data center proxies and residential proxies don\u2019t worry you will be able to watch it here:If you feel like you know enough already and you don't want to spend way too much time on managing proxies, you can just use an already existing solution for "},
{"content": "Generally, there are 3 steps needed to find the best proxy management method for your web scraping project and to make sure you can get data not just today but also in the future, long-term.You need to define the traffic profile first to determine the concrete needs of your project. What is a traffic profile?It includes, first of all, the  that you're trying to get data from. And also if there's any technical challenges needed to be solved, like JS rendering.The traffic profile also includes the , meaning how many requests do you want to or need to make per hour or per day. Also do you have any specific time window for the requests, like, for example you want to make all your requests only during work hours, for some reason. Or is it okay to get the data at night, when there's significantly less traffic hitting the site.Then the last thing in the traffic profile is, . Because sometimes the website displays different content depending on where you are. So you need to use proxies that are in that specific region you need.So these three elements together make the traffic profile: websites, volume and geo locations. Now, with these, you can determine the exact proxy situation that you need a solution for.The next step to scale up is to get a proxy pool. Based on the traffic profile, now you can estimate"},
{"content": "The web is complex and constantly changing. It is one of the reasons why web data extraction can be difficult, especially in the long term. It\u2019s necessary to understand how a website works really well, before you try to extract data. Luckily, there are lots of inspection and code tools available for this and in this article we will show you some of our favorites.All major browsers come packed with a set of development tools. Although these have been built with the goal of building websites in mind, they can also be used to analyze web pages and traffic. These are some pretty powerful tools for working with websites.For Google Chrome, these developer tools can be accessed from any page by right-clicking then choosing 'Inspect' or using the shortcut 'CTRL + shift + I' (or '\u2318 + Option + I' on macs).You can use these tools to perform some basic operations:Most web pages contain a lot of javascript that needs to be executed before you can see the final output. But you can see how the initial request looks before all of this by checking the page source. To do that you can right-click and then click on 'View page source' or use the shortcuts:Press CTRL + U (\u2318 + Option + U) to view sourcePress CTRL + F (\u2318 + Option + F) to searchSometimes the information you're looking for is not loaded with the first request. Or perhaps there is an API call that loads the data you need. In order to \u201ccatch\u201d these calls, you'll want to see what requests are needed to load a certain page. You can find out this and more in the 'Network' tab. This is what it looks like:Of course, you'll also need to know the details on individual requests. If you click on any of these you'll be able to see a lot more information such as request and response headers, cookies and the payload used when sending POST requests for example.Let's take a look at one of the requests needed to load the main page of google."},
{"content": "Price Intelligence is leveraging web data to make better pricing, marketing, and business decisions. Basically, it is all about making use of the available data to optimize your pricing strategy, making it more competitive, increasing profitability, and ultimately, improving your business performance.From competitor monitoring to dynamic pricing and MAP monitoring, web extracted pricing data has endless uses. Brands and e-commerce companies use pricing data to monitor an overall view of the market. Dynamic pricing can be used to make automatic pricing decisions based on competitor\u2019s data combined with internal data so that you always remain profitable. MAP or Minimum Advertising Price monitoring is a technique that uses web extracted data to ensure the resellers and partners are maintaining the pricing according to the company policies.During our webinar on \u201c\u201d in June 2020, we got a lot of questions related to the processes and challenges of pricing data extraction. We cover a few important questions here:A: It varies from website to website, but the general idea is to find the pages where such promotion codes are available and build the logic of looking up code and applying it (clicking a button or sending AJAX request) into your extraction code.A: Websites showcase erroneous pricing data when they detect you scraping regularly. This especially happens when you are trying to scale - i.e scrape a lot of products very frequently. Erroneous pricing is not easily recognizable, but comparing the prices or other data fields with previously extracted data and manually checking if there is a big difference in the extracted data can help.The long-term solution for this would be to be smarter about how you scale and be more thoughtful about the  you use.A: Scraping accurate data is all about having a reliable quality assurance process. The first step towards this process is to have a well-defined JSON schema. Your QA process needs to be a balanced combination of automated ways of testing the data as well as manual ways. This blog post gives a detailed description of ."},
{"content": "We are excited to announce our newest data extraction API. The  is now publicly available as a BETA release.If you want to skip the introductions and just get stuck in, here are the links you need:AutoExtract Comments API sets out to bring the power of our automatic data extraction capabilities currently used for applications such as  and more into the arena of blog comment analysis.\u00a0The underlying data model for the API was released to production as part of 20.6.0 release of AutoExtract.Customer support management presents many challenges due to the sheer number of requests, varied topics, and diverse departments within a company that might have a say in resolving the matter.Sourcing structured data from blog comments as provided by our API can be used in tandem with  solutions to quickly and effectively identify, track and act upon particular conversation strings \u2018hidden\u2019 amongst the noise of thousands of comments. You are effectively highlighting warning signs that your CX team should become involved before an incident takes place.Another particular powerful insight that can be derived from comments revolving around the sphere of Voice of Customer (VoC) and product analysis. By tapping into blog comments, you can search keywords for a particular product or feature or use the parsed data to train sentiment analysis model to find only the information you need."},
{"content": "Imagine a long crawling process, like extracting data from a website for a whole month. We can start it and leave it running until we get the results. Though, we can agree that a whole month is plenty of time for something to go wrong. The target website can go down for a few minutes/hours, there can be some sort of power outage in your crawling server or even some other internet connection issues.Any of those are real case scenarios and can happen at any given moment, bringing risk to your data extraction pipeline. In this case, if something like that happens, you may need to restart your crawling process and wait even longer to get access to that precious data. But, you don\u2019t need to panic, this is where (HCF) comes to our rescue.HCF is an API to store request data and is available through Scrapy Cloud projects. It is a bit similar to , but its intended use is to store request data, not a generic key value storage like Collections. At this moment, if you are familiar with , you may be wondering why one would use HCF, when Scrapy can store and recover the crawling state by itself.\u00a0The advantage is that Scrapy requires you to manage this state, by saving the content to disk (so needs disk quota) and if you are running inside a container, like in Scrapy Cloud, local files are lost once the process is finished. So, having some kind of external storage for requests is an alternative that takes this burden from your shoulders, leaving you to think about the extraction logic and not about the details on how to proceed in case it crashes and you need to restart.Before digging into an example of how to use HCF, I\u2019ll go over a bit on how it is structured. We can create many Frontiers per project, for each one we need a name. These Frontiers are then broken into slots, something similar to sharding, that can be useful in a producer-consumer scenario (topic of one of our upcoming blog posts). Usually, the name will be the name of the spider, to avoid any confusion. The catchy part is that we shouldn't change the number of slots after it was created, so keep it in mind when creating it.Now that we know what HCF is and how we could make use of it, it is time to see it working. For this purpose, we\u2019ll build a simple Scrapy spider to extract book information from . To get started, we\u2019ll create a new scrapy project and install the proper dependencies as shown below (type them in your terminal)."},
{"content": "As the internet continues to grow, the amount of data it generates grows with it, opening new opportunities to improve processes and make more informed decisions. Real estate is one of the many industries that are being disrupted by data-related technologies and innovations. Whether you are a broker, realtor, investor, or property manager you have the potential to become data-driven and gain invaluable insights from web extracted data.In this article, you will see the many ways real estate data can help you and how utilizing web scraping can help you and your organization become disruption-proofed and fully prepared for the world of tomorrow.There is more public data available in the real estate market than ever. There are numerous listing sites, endless data points available for everyone to see. And if there\u2019s data, there should be a way to  from the data to make better decisions. But there\u2019s one big problem...Unfortunately, many websites don\u2019t provide . Or even if they do, you might not get all the data you want only in a limited fashion. But still, the publicly available data is there, you just don\u2019t have a straightforward way to get the data. This is where web data extraction comes in. Web data extraction allows you to get this publicly available real estate data at scale. Using the correct tools or partnering with a , like Scrapinghub, allows you to tap into the world of web scraping and enjoy the benefits of .There are many situations where estimating the value of a property is necessary. Maybe you\u2019re trying to list it online for the most accurate price, maybe you\u2019re trying to get financing or you\u2019re analyzing a property before purchasing. You want to get the most accurate value of how much the property is worth.Being in the real estate market means that you have a lot of competition. In order to be ahead of the competition, you need to find ways to know more than others. As most realtors get their data from a single listing like the MLS, you can differentiate yourself by accessing alternative data sources. Web data extraction can help by allowing you to fetch structured "}
]